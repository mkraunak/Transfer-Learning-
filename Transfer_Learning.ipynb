{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nHWDriYvVDEZ"
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npqsQN6e8FZw"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3dWP6V7sRta"
   },
   "outputs": [],
   "source": [
    "!pip install torchfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnmISsCes3I4"
   },
   "outputs": [],
   "source": [
    "## Below is the code to extract weights from the VGG_FACE.t7 file given.\n",
    "## Reference taken from https://github.com/prlz77/vgg-face.pytorch/blob/master/models/vgg_face.py\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchfile\n",
    "\n",
    "class VGG_16(nn.Module):\n",
    "    \"\"\"\n",
    "    Main Class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.block_size = [2, 2, 3, 3, 3]\n",
    "        self.conv_1_1 = nn.Conv2d(3, 64, 3, stride=1, padding=1)\n",
    "        self.conv_1_2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        self.conv_2_1 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
    "        self.conv_2_2 = nn.Conv2d(128, 128, 3, stride=1, padding=1)\n",
    "        self.conv_3_1 = nn.Conv2d(128, 256, 3, stride=1, padding=1)\n",
    "        self.conv_3_2 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.conv_3_3 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.conv_4_1 = nn.Conv2d(256, 512, 3, stride=1, padding=1)\n",
    "        self.conv_4_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.conv_4_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.conv_5_1 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.conv_5_2 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.conv_5_3 = nn.Conv2d(512, 512, 3, stride=1, padding=1)\n",
    "        self.fc6 = nn.Linear(512 * 7 * 7, 4096)\n",
    "        self.fc7 = nn.Linear(4096, 4096)\n",
    "        self.fc8 = nn.Linear(4096, 2622)\n",
    "\n",
    "    def load_weights(self, path=\"/content/vgg_face_torch/VGG_FACE.t7\"):\n",
    "        \"\"\" Function to load luatorch pretrained\n",
    "        Args:\n",
    "            path: path for the luatorch pretrained\n",
    "        \"\"\"\n",
    "        model = torchfile.load(path)\n",
    "        counter = 1\n",
    "        block = 1\n",
    "        for i, layer in enumerate(model.modules):\n",
    "            if layer.weight is not None:\n",
    "                if block <= 5:\n",
    "                    self_layer = getattr(self, \"conv_%d_%d\" % (block, counter))\n",
    "                    counter += 1\n",
    "                    if counter > self.block_size[block - 1]:\n",
    "                        counter = 1\n",
    "                        block += 1\n",
    "                    self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n",
    "                    self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n",
    "                else:\n",
    "                    self_layer = getattr(self, \"fc%d\" % (block))\n",
    "                    block += 1\n",
    "                    self_layer.weight.data[...] = torch.tensor(layer.weight).view_as(self_layer.weight)[...]\n",
    "                    self_layer.bias.data[...] = torch.tensor(layer.bias).view_as(self_layer.bias)[...]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Pytorch forward\n",
    "        Args:\n",
    "            x: input image (224x224)\n",
    "        Returns: class logits\n",
    "        \"\"\"\n",
    "        x = F.relu(self.conv_1_1(x))\n",
    "        x = F.relu(self.conv_1_2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv_2_1(x))\n",
    "        x = F.relu(self.conv_2_2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv_3_1(x))\n",
    "        x = F.relu(self.conv_3_2(x))\n",
    "        x = F.relu(self.conv_3_3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv_4_1(x))\n",
    "        x = F.relu(self.conv_4_2(x))\n",
    "        x = F.relu(self.conv_4_3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv_5_1(x))\n",
    "        x = F.relu(self.conv_5_2(x))\n",
    "        x = F.relu(self.conv_5_3(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = F.dropout(x, 0.5, self.training)\n",
    "        x = F.relu(self.fc7(x))\n",
    "        x = F.dropout(x, 0.5, self.training)\n",
    "        return self.fc8(x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  model = VGG_16().double()\n",
    "\n",
    "    # # print(model)\n",
    "  torch.save(model,'VGG_Face_pytorch.pt')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsEwfAWi0K0v"
   },
   "outputs": [],
   "source": [
    "model = torch.load('VGG_Face_pytorch.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ooMkqRI0Sim"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJrpfVMS5AOW"
   },
   "outputs": [],
   "source": [
    "## Loading weights from pytorch model in list\n",
    "## list weights_pytorch conatins weights\n",
    "##list bias_pytorch contains biases\n",
    "weights_pytorch=[]\n",
    "bias_pytorch=[]\n",
    "for l in model.children():\n",
    "  weights_pytorch.append(l.weight.detach().numpy())\n",
    "  bias_pytorch.append(l.bias.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCrekYjzU242"
   },
   "outputs": [],
   "source": [
    "## Classifier Model to distinguish between Male and Female\n",
    "## VGG16 model has been used here\n",
    "## Used Batch Normalization and drop out to increase model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8T4ifcLxK3X"
   },
   "outputs": [],
   "source": [
    "import keras,os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers.normalization import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoFBOjLXJJpQ"
   },
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Conv2D(input_shape=(64,64,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model2.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model2.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model2.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model2.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model2.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(units=4096,activation=\"relu\"))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units=4096,activation=\"relu\"))\n",
    "model2.add(Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fRe_bZGSxab2"
   },
   "outputs": [],
   "source": [
    "# parameters for the training of the model\n",
    "# neural netwrok tries to minimize the loss, loss is relationship to accuarcy \n",
    "\n",
    "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfJSIqqxv32u"
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVP_pSGWyVqk"
   },
   "outputs": [],
   "source": [
    "# Loading the indexes of trainable layers in list l1\n",
    "l1=[]\n",
    "for i in model2.layers:\n",
    "  if not \"batch_normalization\" in str(i.name):\n",
    "    if i.trainable_weights:\n",
    "      l1.append((model2.layers.index(i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ2rNly93BN7"
   },
   "outputs": [],
   "source": [
    "## Transferring weights obtained from the given file to model made in keras(VGG16)\n",
    "for indx,vgg_l in enumerate(l1[:-3]):\n",
    "\n",
    "  w = weights_pytorch[indx]\n",
    "  b = bias_pytorch[indx]\n",
    "  \n",
    "  if len(w.shape) < 4:\n",
    "    w_t = w.transpose(1,0)\n",
    "  else:\n",
    "    w_t = w.transpose(2,3,1,0)\n",
    "\n",
    "  model2.layers[vgg_l].set_weights([w_t, b])\n",
    "  \n",
    "  model2.layers[vgg_l].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwCs328BUS0R"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " Path to given datasets: aligned and valid folders\n",
    "\"\"\"\n",
    "path_aligned='/content/drive/My Drive/combined/aligned'\n",
    "path_valid='/content/drive/My Drive/combined/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bPBQQguVXeN"
   },
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "# creating generator to yield training data, validation data in batches\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import array_to_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAra13xqg52e"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "# Mapping for male or female to labels 0 or 1\n",
    "gender_mapping={'F':0,'M':1}\n",
    "\n",
    "def list_paths(root_dir):\n",
    "  \"\"\"\n",
    "  This function generates list of image files looping\n",
    "  through each directory. It generates random \n",
    "  files of given sample size from each directory.\n",
    "  \"\"\"\n",
    "  train_paths=[]\n",
    "  valid_paths=[]\n",
    "  test_paths=[]\n",
    "\n",
    "  # Listing all directories in the root path\n",
    "  dirs=os.listdir(root_dir)\n",
    "\n",
    "  # Looping through all directories to generate list of files\n",
    "  for each in dirs:\n",
    "\n",
    "    # Limiting the number of files using start and end\n",
    "    files_all = os.listdir(root_dir+\"/\"+each)\n",
    "    len_files= len(files_all)\n",
    "    \n",
    "\n",
    "    # If there are no files present in the directory, then skip   \n",
    "    if len_files>0:\n",
    "      random.shuffle(files_all)\n",
    "      train_files = files_all[0:int(0.5*len_files)-1]\n",
    "      valid_files = files_all[int(0.5*len_files):int(0.75*len_files-1)]\n",
    "      test_files = files_all[int(0.75*len_files):len_files-1]  \n",
    "\n",
    "      for file in train_files:\n",
    "        train_paths.append(root_dir+\"/\"+each+\"/\"+file)\n",
    "\n",
    "      for file in valid_files:\n",
    "        valid_paths.append(root_dir+\"/\"+each+\"/\"+file)\n",
    "       \n",
    "      for file in test_files:\n",
    "        test_paths.append(root_dir+\"/\"+each+\"/\"+file)\n",
    "\n",
    "  return train_paths, valid_paths, test_paths\n",
    "\n",
    "def read_img(path):\n",
    "  \"\"\"\n",
    "  Function to read input image from the path.\n",
    "  \"\"\"\n",
    "  img = load_img(path)\n",
    "  img_array = img_to_array(img)\n",
    "  return img_array\n",
    "\n",
    "def preprocessing(img):\n",
    "  \"\"\"\n",
    "  Function to preprocess the image\n",
    "  \"\"\"\n",
    "  # Resize the image to 64x64\n",
    "  #img_resized = tf.image.resize(img, (64,64), preserve_aspect_ratio=False)\n",
    "  img_resized= tf.keras.preprocessing.image.smart_resize(img, (64,64))\n",
    "\n",
    "  # Normalize image\n",
    "  img_normalized = img_resized/255.0 #tf.image.per_image_standardization(img_resized)\n",
    "  \n",
    "  return img_normalized\n",
    "\n",
    "def image_generator(files, batch_size):\n",
    "    \"\"\"\n",
    "    Generator function to generate batches of images\n",
    "    using the list of image paths for training, validation\n",
    "    and testing.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "          # Select files (paths/indices) for the batch\n",
    "          batch_paths  = np.random.choice(a=files, \n",
    "                                          size = batch_size)\n",
    "          batch_input  = []\n",
    "          batch_output = [] \n",
    "          \n",
    "          # Read in each input, perform preprocessing and get labels\n",
    "          for input_path in batch_paths:\n",
    "              input = read_img(input_path)\n",
    "              output = gender_mapping[input_path.split(\"/\")[-2].split(\"_\")[1]]\n",
    "            \n",
    "              input = preprocessing(input)\n",
    "              batch_input += [ input ]\n",
    "              batch_output += [ output ]\n",
    "\n",
    "          # Return a tuple of (input, output) to feed the network\n",
    "          batch_x = np.array( batch_input )\n",
    "          batch_y = np.array( batch_output )\n",
    "          \n",
    "          yield( batch_x, batch_y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MA3JyhHNimrh"
   },
   "outputs": [],
   "source": [
    "## training data, validation data and testing data generated by generator in batches\n",
    "\n",
    "x_train, x_valid, x_test=list_paths(path_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afDfxSn_ShiD"
   },
   "outputs": [],
   "source": [
    "print(len(x_train), len(x_valid), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRTkCDiYnK3K"
   },
   "outputs": [],
   "source": [
    "#Training of Vgg16 geneder classifier\n",
    "model2.fit_generator(image_generator(x_train,128),steps_per_epoch = math.ceil(len(x_train)/128),epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Su0KncNcR93E"
   },
   "outputs": [],
   "source": [
    "#Evaluation on validation dataset\n",
    "\n",
    "validation=next(image_generator(x_valid,7210))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZlzCfJ-jLY1"
   },
   "outputs": [],
   "source": [
    "# cross validation\n",
    "model2.evaluate(x=validation[0], y=validation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DPxVe_Qym0UP"
   },
   "outputs": [],
   "source": [
    "# Evaluation on test set\n",
    "test=next(image_generator(x_valid,7268))\n",
    "model2.evaluate(x=test[0], y=test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1j19FGz6BiTc"
   },
   "outputs": [],
   "source": [
    "# Calling `save('my_model')` creates a SavedModel folder of vgg16 classifier model\n",
    "from keras.models import load_model\n",
    "model2.save('my_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2yXrGzS0CJ1n"
   },
   "outputs": [],
   "source": [
    "#saving vgg16 classifier model weights\n",
    "model2.save_weights('my_model_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model\n",
    "from keras.models import load_model\n",
    "model2 = load_model(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYM7LK3XokEB"
   },
   "outputs": [],
   "source": [
    "# Prediction of model on test data\n",
    "y_pred = model2.predict(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ppn2AoY2AfjJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Creates a confusion matrix\n",
    "y_pred = np.round(model2.predict(test[0]))\n",
    "y_test=test[1]\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "\n",
    "cm_df = pd.DataFrame(cm,\n",
    "                     index = ['female','male'], \n",
    "                     columns = ['female','male'])\n",
    "\n",
    "plt.figure(figsize=(5.5,4))\n",
    "sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DhHC3KLrw4OC"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "# calculate recall\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "print('Recall: %.3f' % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q47iKzvdz7jk"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "# calculate prediction\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "print('Precision: %.3f' % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rcCu9ku0JMb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "# calculate score\n",
    "score = f1_score(y_test, y_pred, average='binary')\n",
    "print('F-Measure: %.3f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLevWj4l1_OE"
   },
   "outputs": [],
   "source": [
    "! pip install scikit-plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jretsVIG09BE"
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.rcParams[\"figure.figsize\"] = (7,7)\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "predictions=model2.predict(test[0])\n",
    "fpr, tpr, thresholds = roc_curve(y_test,predictions)\n",
    "auc_score = roc_auc_score(y_test,predictions)\n",
    "\n",
    "plt.plot(fpr,tpr,label='Vgg16_model')\n",
    "plt.plot(fpr,fpr,label='Random guessing')\n",
    "plt.title('ROC score of Vgg16 = %2.2f'%auc_score)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Matroid_Test.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
